# search-engine-core

[![CI/CD Pipeline](https://github.com/hatefsystems/search-engine-core/actions/workflows/main.yml/badge.svg?branch=master)](https://github.com/hatefsystems/search-engine-core/actions/workflows/main.yml)

We are working toward a future where internet search is more open, reliable, and
aligned with the values and needs of people everywhere. This community-oriented
initiative encourages inclusive participation and shared benefit, aiming to
complement existing structures by enhancing access, strengthening privacy
protections, and promoting constructive global collaboration. Together, we can
help shape a digital environment that is transparent, respectful, and supportive
of all.

# Search Engine Core

A high-performance search engine built with C++, uWebSockets, MongoDB, and Redis
with comprehensive logging, testing infrastructure, modern controller-based
routing system, **advanced session-based crawler management**, **intelligent
SPA rendering capabilities** for JavaScript-heavy websites, and **reusable
template system** for common crawling patterns.

## âš¡ **Latest Performance Optimizations**

### **Speed Improvements (50-70% Faster)**

- **Render Time**: 8-12 seconds per page (vs 22-24 seconds before)
- **Wait Times**: 8s network idle, 2s simple wait (60% faster)
- **Timeouts**: 15s max SPA rendering (50% faster)
- **Concurrent Sessions**: 10 Chrome instances (100% more)
- **Memory**: 2GB allocation (100% more)

### **JavaScript Minification & Caching (99.6% Faster)**

- **Redis-based caching**: 98.7% faster subsequent requests (2ms vs 150ms)
- **Browser caching**: 1-year cache with immutable flag for static assets
- **Cache hit rate**: 90%+ for returning users
- **Server load reduction**: 90%+ for cached JavaScript files
- **Production headers**: Industry-standard caching headers with ETags

### **Expected Performance**

- **Before**: 3+ minutes for 5 pages
- **After**: 1-2 minutes for 5 pages (50-70% faster)
- **JavaScript caching**: 99.6% faster for cached files (0.17ms vs 43.31ms)

## Key Features

### ðŸš€ **Advanced Session-Based Web Crawling with SPA Support**

- **Intelligent Session Management**: Advanced CrawlerManager with session-based
  crawl orchestration and control
- **Concurrent Session Support**: Multiple independent crawl sessions with
  individual tracking and management
- **Flexible Session Control**: Optional stopping of previous sessions for
  resource management (`stopPreviousSessions`)
- **Intelligent SPA Detection**: Automatically detects React, Vue, Angular, and
  other JavaScript frameworks
- **Headless Browser Rendering**: Full JavaScript execution using
  browserless/Chrome for dynamic content
- **Enhanced Text Extraction**: Configurable text content extraction with
  `extractTextContent` parameter
- **Re-crawl Capabilities**: Force re-crawling of existing sites with the
  `force` parameter
- **Title Extraction**: Properly extracts titles from JavaScript-rendered pages
  (e.g., www.digikala.com)
- **Configurable Content Storage**: Support for full content extraction with
  `includeFullContent` parameter
- **Optimized Timeouts**: 15-second default timeout for complex JavaScript sites
  (50% faster)
- **Durable Frontier (Kafka-backed)**: At-least-once delivery using Apache Kafka
  with direct `librdkafka` client; restart-safe via MongoDB `frontier_tasks`
  state; admin visibility of URL states

### ðŸŽ¯ **Modern Session-Aware API Architecture**

- **Session-Based Crawler API**: Enhanced `/api/crawl/add-site` with session ID
  responses and management
- **Crawl Session Monitoring**: Real-time session status tracking with
  `/api/crawl/status`
- **Session Details API**: Comprehensive session information via
  `/api/crawl/details`
- **SPA Render API**: Direct `/api/spa/render` endpoint for on-demand JavaScript
  rendering
- **Unified Content Storage**: Seamlessly handles both static HTML and
  SPA-rendered content
- **Flexible Configuration**: Runtime configuration of SPA rendering, timeouts,
  and content extraction

### ðŸš€ **Advanced JavaScript Minification & Caching System**

- **Microservice Architecture**: Dedicated Node.js minification service with Terser
- **Redis-based Caching**: 98.7% faster subsequent requests (2ms vs 150ms)
- **Production Caching Headers**: 1-year browser cache with immutable flag
- **Content-based ETags**: Automatic cache invalidation when files change
- **Cache Monitoring**: Real-time cache statistics via `/api/cache/stats`
- **Graceful Fallbacks**: Memory cache when Redis unavailable
- **Size-based Optimization**: JSON payload (â‰¤100KB) vs file upload (>100KB)
- **Thread-safe Operations**: Concurrent request handling with mutex protection


### ðŸŽ¯ **Reusable Template System for Common Crawling Patterns**

- **7 Pre-built Templates**: News, e-commerce, blog, corporate, documentation, forum, social media
- **Standardized Configurations**: Site-specific crawling parameters and CSS selectors
- **Template Management API**: Full CRUD operations for template creation and management
- **Template Application**: Easy integration with existing crawl API via `/api/crawl/add-site-with-template`
- **Backward Compatibility**: Existing crawl API continues to work unchanged
- **Performance Optimized**: Singleton registry with efficient template lookup and application
- **Validation & Error Handling**: Comprehensive input validation with clear error messages
- **Persistence**: Automatic template saving to disk with directory-based storage
=======
### ðŸ’° **Sponsor Management API**

- **MongoDB Integration**: Direct database storage with proper C++ driver initialization
- **Bank Information Response**: Complete Iranian bank details for payment processing
- **Data Validation**: Comprehensive input validation for all sponsor fields
- **Backend Tracking**: Automatic capture of IP, user agent, and submission timestamps
- **Status Management**: Support for PENDING, VERIFIED, REJECTED, CANCELLED states
- **Error Handling**: Graceful fallbacks with detailed error logging
- **Frontend Integration**: JavaScript form handling with success/error notifications


## Project Structure

```
.
â”œâ”€â”€ .github/workflows/          # GitHub Actions workflows
â”‚   â”œâ”€â”€ docker-build.yml       # Main build orchestration
â”‚   â”œâ”€â”€ docker-build-drivers.yml   # MongoDB drivers build
â”‚   â”œâ”€â”€ docker-build-server.yml    # MongoDB server build
â”‚   â””â”€â”€ docker-build-app.yml       # Application build
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ controllers/            # Controller-based routing system
â”‚   â”‚   â”œâ”€â”€ HomeController.cpp  # Home page, sponsor API, and coming soon handling
â”‚   â”‚   â”œâ”€â”€ SearchController.cpp # Search functionality and crawler APIs
â”‚   â”‚   â”œâ”€â”€ StaticFileController.cpp # Static file serving with caching
â”‚   â”‚   â”œâ”€â”€ CacheController.cpp # Cache monitoring and management
â”‚   â”‚   â””â”€â”€ TemplatesController.cpp # Template system API endpoints
â”‚   â”œâ”€â”€ routing/                # Routing infrastructure
â”‚   â”‚   â”œâ”€â”€ Controller.cpp      # Base controller class with route registration
â”‚   â”‚   â””â”€â”€ RouteRegistry.cpp   # Central route registry singleton
â”‚   â”œâ”€â”€ common/                 # Shared utilities
â”‚   â”‚   â”œâ”€â”€ Logger.cpp          # Centralized logging implementation
â”‚   â”‚   â””â”€â”€ JsMinifierClient.cpp # JavaScript minification microservice client
â”‚   â”œâ”€â”€ crawler/                # Advanced web crawling with SPA support
â”‚   â”‚   â”œâ”€â”€ PageFetcher.cpp     # HTTP fetching with SPA rendering integration
â”‚   â”‚   â”œâ”€â”€ BrowserlessClient.cpp # Headless browser client for SPA rendering
â”‚   â”‚   â”œâ”€â”€ Crawler.cpp         # Main crawler with SPA detection and processing
â”‚   â”‚   â”œâ”€â”€ RobotsTxtParser.cpp # Robots.txt parsing with rule logging
â”‚   â”‚   â”œâ”€â”€ URLFrontier.cpp     # URL queue management with frontier logging
â”‚   â”‚   â”œâ”€â”€ templates/          # Template system for common crawling patterns
â”‚   â”‚   â”‚   â”œâ”€â”€ TemplateTypes.h # Core template data structures
â”‚   â”‚   â”‚   â”œâ”€â”€ TemplateRegistry.h # Singleton template registry
â”‚   â”‚   â”‚   â”œâ”€â”€ TemplateValidator.h # Input validation and normalization
â”‚   â”‚   â”‚   â”œâ”€â”€ TemplateApplier.h # Template application to CrawlConfig
â”‚   â”‚   â”‚   â”œâ”€â”€ TemplateStorage.h # JSON persistence and file operations
â”‚   â”‚   â”‚   â””â”€â”€ PrebuiltTemplates.h # Pre-built template definitions
â”‚   â”‚   â””â”€â”€ models/             # Data models and configuration
â”‚   â”‚       â”œâ”€â”€ CrawlConfig.h   # Enhanced configuration with SPA parameters
â”‚   â”‚       â””â”€â”€ CrawlResult.h   # Crawl result structure
â”‚   â”œâ”€â”€ search_core/            # Search API implementation
â”‚   â”‚   â”œâ”€â”€ SearchClient.cpp    # RedisSearch interface with connection pooling
â”‚   â”‚   â”œâ”€â”€ QueryParser.cpp     # Query parsing with AST generation
â”‚   â”‚   â””â”€â”€ Scorer.cpp          # Result ranking and scoring configuration
â”‚   â””â”€â”€ storage/                # Data persistence with comprehensive logging
â”‚       â”œâ”€â”€ MongoDBStorage.cpp  # MongoDB operations with CRUD logging
â”‚       â”œâ”€â”€ RedisSearchStorage.cpp # Redis search indexing with operation logging
â”‚       â”œâ”€â”€ ContentStorage.cpp  # Unified storage with detailed flow logging
â”‚       â””â”€â”€ SponsorStorage.cpp  # Sponsor data management with MongoDB integration
â”œâ”€â”€ js-minifier-service/        # JavaScript minification microservice
â”‚   â”œâ”€â”€ enhanced-server.js      # Enhanced minification server with multiple methods
â”‚   â”œâ”€â”€ package.json           # Node.js dependencies
â”‚   â””â”€â”€ Dockerfile             # Container configuration
â”œâ”€â”€ scripts/                   # Utility scripts
â”‚   â”œâ”€â”€ test_js_cache.sh       # JavaScript caching test script
â”‚   â””â”€â”€ minify_js_file.sh      # JS minification utility
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ routing/                # Routing system headers
â”‚   â”œâ”€â”€ Logger.h                # Logging interface with multiple levels
â”‚   â”œâ”€â”€ search_core/            # Search API headers
â”‚   â”œâ”€â”€ mongodb.h               # MongoDB singleton instance management
â”‚   â””â”€â”€ search_engine/          # Public API headers
â”‚       â”œâ”€â”€ crawler/            # Public crawler API (new)
â”‚       â”‚   â”œâ”€â”€ BrowserlessClient.h
â”‚       â”‚   â”œâ”€â”€ PageFetcher.h
â”‚       â”‚   â”œâ”€â”€ Crawler.h
â”‚       â”‚   â”œâ”€â”€ CrawlerManager.h
â”‚       â”‚   â””â”€â”€ models/
â”‚       â”‚      â”œâ”€â”€ CrawlConfig.h
â”‚       â”‚      â”œâ”€â”€ CrawlResult.h
â”‚       â”‚      â””â”€â”€ FailureType.h
â”‚       â””â”€â”€ storage/            # Storage API headers
â”‚          â”œâ”€â”€ SponsorProfile.h # Sponsor data model
â”‚          â””â”€â”€ SponsorStorage.h # Sponsor storage interface
â”œâ”€â”€ docs/                       # Comprehensive documentation
â”‚   â”œâ”€â”€ SPA_RENDERING.md        # SPA rendering setup and usage guide
â”‚   â”œâ”€â”€ content-storage-layer.md # Storage architecture documentation
â”‚   â”œâ”€â”€ SCORING_AND_RANKING.md  # Search ranking algorithms
â”‚   â”œâ”€â”€ development/            # Development guides
â”‚   â”‚   â””â”€â”€ MONGODB_CPP_GUIDE.md # MongoDB C++ development patterns
â”‚   â””â”€â”€ api/                    # REST API documentation
â”‚      â”œâ”€â”€ sponsor_endpoint.md  # Sponsor API documentation
â”‚      â””â”€â”€ README.md            # API overview
â”œâ”€â”€ pages/                      # Frontend source files
â”œâ”€â”€ public/                     # Static files served by server
â”œâ”€â”€ tests/                      # Comprehensive testing suite
â”‚   â”œâ”€â”€ crawler/                # Crawler component tests (including SPA tests)
â”‚   â”œâ”€â”€ search_core/            # Search API unit tests
â”‚   â””â”€â”€ storage/                # Storage component tests
â”œâ”€â”€ config/                     # Configuration files
â”‚   â””â”€â”€ templates/              # Template system configuration
â”‚       â”œâ”€â”€ news-site.json      # News website template
â”‚       â”œâ”€â”€ ecommerce-site.json # E-commerce template
â”‚       â”œâ”€â”€ blog-site.json      # Blog template
â”‚       â”œâ”€â”€ corporate-site.json # Corporate website template
â”‚       â”œâ”€â”€ documentation-site.json # Documentation template
â”‚       â”œâ”€â”€ forum-site.json     # Forum template
â”‚       â””â”€â”€ social-media.json   # Social media template
â”œâ”€â”€ examples/                   # Usage examples
â”‚   â””â”€â”€ spa_crawler_example.cpp # SPA crawling example
â”œâ”€â”€ docker-compose.yml          # Development multi-service orchestration
â””â”€â”€ docker-compose.prod.yml     # Production deployment (uses GHCR images)
```

## Enhanced Crawler API

### `/api/crawl/add-site` - Advanced Session-Based Crawling Endpoint

**Enhanced Parameters:**

| Parameter              | Type    | Default                   | Description                                        |
| ---------------------- | ------- | ------------------------- | -------------------------------------------------- |
| `url`                  | string  | required                  | Target URL to crawl                                |
| `maxPages`             | integer | 1000                      | Maximum pages to crawl                             |
| `maxDepth`             | integer | 3                         | Maximum crawl depth                                |
| `force`                | boolean | true                      | Force re-crawl even if already crawled             |
| `extractTextContent`   | boolean | true                      | Extract and store full text content                |
| `stopPreviousSessions` | boolean | false                     | Stop all active sessions before starting new crawl |
| `spaRenderingEnabled`  | boolean | true                      | Enable SPA detection and rendering                 |
| `includeFullContent`   | boolean | false                     | Store full content (like SPA render API)           |
| `browserlessUrl`       | string  | "http://browserless:3000" | Browserless service URL                            |
| `restrictToSeedDomain` | boolean | true                      | Limit crawling to seed domain                      |
| `followRedirects`      | boolean | true                      | Follow HTTP redirects                              |
| `maxRedirects`         | integer | 10                        | Maximum redirects to follow                        |

**Session Management Options:**

- **`stopPreviousSessions: false`** (Default): Allows concurrent crawling
  sessions
- **`stopPreviousSessions: true`**: Stops all active sessions before starting
  new crawl (useful for resource management)

**Example Request:**

```json
POST /api/crawl/add-site
{
  "url": "https://www.digikala.com",
  "maxPages": 100,
  "maxDepth": 2,
  "force": true,
  "extractTextContent": true,
  "stopPreviousSessions": false,
  "spaRenderingEnabled": true,
  "includeFullContent": true,
  "browserlessUrl": "http://browserless:3000"
}
```

**Success Response:**

```json
{
  "success": true,
  "message": "Crawl session started successfully",
  "data": {
    "sessionId": "crawl_1643123456789_001",
    "url": "https://www.digikala.com",
    "maxPages": 100,
    "maxDepth": 2,
    "force": true,
    "extractTextContent": true,
    "stopPreviousSessions": false,
    "spaRenderingEnabled": true,
    "includeFullContent": true,
    "browserlessUrl": "http://browserless:3000",
    "status": "starting"
  }
}
```

### Session Management APIs

#### `/api/crawl/status` - Session Status Monitoring

**Parameters:**

- `sessionId` (string): Session ID returned from `/api/crawl/add-site`

**Example:**

```bash
GET /api/crawl/status?sessionId=crawl_1643123456789_001
```

**Response:**

```json
{
  "success": true,
  "sessionId": "crawl_1643123456789_001",
  "status": "running",
  "pagesProcessed": 45,
  "totalPages": 100
}
```

#### `/api/crawl/details` - Comprehensive Session Information

**Parameters:**

- `sessionId` (string): Session ID for detailed information
- `url` (string): Alternative lookup by URL

**Example:**

```bash
GET /api/crawl/details?sessionId=crawl_1643123456789_001
```

### `/api/spa/render` - Direct SPA Rendering

**Parameters:**

| Parameter            | Type    | Default  | Description                       |
| -------------------- | ------- | -------- | --------------------------------- |
| `url`                | string  | required | URL to render                     |
| `timeout`            | integer | 30000    | Rendering timeout in milliseconds |
| `includeFullContent` | boolean | false    | Include full rendered HTML        |

**Example Usage:**

```json
POST /api/spa/render
{
  "url": "https://www.digikala.com",
  "timeout": 60000,
  "includeFullContent": true
}
```

**Success Response:**

```json
{
  "success": true,
  "url": "https://www.digikala.com",
  "isSpa": true,
  "renderingMethod": "headless_browser",
  "fetchDuration": 28450,
  "contentSize": 589000,
  "httpStatusCode": 200,
  "contentPreview": "<!DOCTYPE html>...",
  "content": "<!-- Full rendered HTML when includeFullContent=true -->"
}
```

### `/api/v2/sponsor-submit` - Sponsor Application Submission

**Parameters:**

| Parameter | Type   | Required | Description                  |
| --------- | ------ | -------- | ---------------------------- |
| `name`    | string | âœ…       | Full name of the sponsor     |
| `email`   | string | âœ…       | Email address for contact    |
| `mobile`  | string | âœ…       | Mobile phone number          |
| `tier`    | string | âœ…       | Sponsorship tier/plan        |
| `amount`  | number | âœ…       | Amount in IRR (Iranian Rial) |
| `company` | string | âŒ       | Company name (optional)      |

**Example Usage:**

```json
POST /api/v2/sponsor-submit
{
  "name": "Ahmad Mohammadi",
  "email": "ahmad@example.com",
  "mobile": "09123456789",
  "tier": "premium",
  "amount": 2500000,
  "company": "Tech Corp"
}
```

**Success Response:**

```json
{
  "success": true,
  "message": "ÙØ±Ù… Ø­Ù…Ø§ÛŒØª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø±Ø³Ø§Ù„ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯",
  "submissionId": "68b05d4abb79f500190b8a92",
  "savedToDatabase": true,
  "bankInfo": {
    "bankName": "Ø¨Ø§Ù†Ú© Ù¾Ø§Ø³Ø§Ø±Ú¯Ø§Ø¯",
    "accountNumber": "3047-9711-6543-2",
    "iban": "IR64 0570 3047 9711 6543 2",
    "accountHolder": "Ù‡Ø§ØªÙ Ù¾Ø±ÙˆÚ˜Ù‡",
    "swift": "PASAIRTHXXX",
    "currency": "IRR"
  },
  "note": "Ù„Ø·ÙØ§Ù‹ Ù¾Ø³ Ø§Ø² ÙˆØ§Ø±ÛŒØ² Ù…Ø¨Ù„ØºØŒ Ø±Ø³ÛŒØ¯ Ù¾Ø±Ø¯Ø§Ø®Øª Ø±Ø§ Ø¨Ù‡ Ø¢Ø¯Ø±Ø³ Ø§ÛŒÙ…ÛŒÙ„ sponsors@hatef.ir Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒØ¯."
}
```

## SPA Rendering Architecture

### Intelligent SPA Detection

The crawler automatically detects Single Page Applications using:

1. **Framework Detection**: React, Vue, Angular, Ember, Svelte patterns
2. **DOM Patterns**: `data-reactroot`, `ng-*`, `v-*` attributes
3. **Content Analysis**: Script-heavy pages with minimal HTML
4. **State Objects**: `window.__initial_state__`, `window.__data__`

### Headless Browser Integration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP/JSON    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   C++ Crawler   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚  Browserless/    â”‚
â”‚                 â”‚                 â”‚  Chrome          â”‚
â”‚  PageFetcher    â”‚                 â”‚                  â”‚
â”‚  + SPA Detect   â”‚                 â”‚  Headless Chrome â”‚
â”‚  + Content Ext  â”‚                 â”‚  + JS Execution  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Performance Optimizations

- **30-second default timeout** for complex JavaScript sites
- **Selective rendering** - only for detected SPAs
- **Content size optimization** - preview vs full content modes
- **Connection pooling** to browserless service
- **Graceful fallback** to static HTML if rendering fails

## Template System API

### Overview

The Template System provides reusable configurations for common crawling patterns, enabling developers to standardize crawler behavior across similar site types.

### Available Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/templates` | List all available templates |
| `GET` | `/api/templates/:name` | Get specific template configuration |
| `POST` | `/api/templates` | Create new custom template |
| `DELETE` | `/api/templates/:name` | Delete template |
| `POST` | `/api/crawl/add-site-with-template` | Crawl using template configuration |

### Pre-built Templates

The system includes 7 pre-built templates optimized for common site types:

- **`news-site`**: News websites and online publications
- **`ecommerce-site`**: Online stores and product listings  
- **`blog-site`**: Personal blogs and content management systems
- **`corporate-site`**: Business websites and corporate pages
- **`documentation-site`**: Technical documentation and API references
- **`forum-site`**: Discussion forums and community sites
- **`social-media`**: Social platforms and user-generated content

### Template Structure

```json
{
  "name": "news-site",
  "description": "Template for news websites",
  "config": {
    "maxPages": 500,
    "maxDepth": 3,
    "spaRenderingEnabled": true,
    "extractTextContent": true,
    "politenessDelay": 1000
  },
  "patterns": {
    "articleSelectors": ["article", ".post", ".story"],
    "titleSelectors": ["h1", ".headline", ".title"],
    "contentSelectors": [".content", ".body", ".article-body"]
  }
}
```

### Usage Examples

#### List Available Templates

```bash
curl http://localhost:3000/api/templates
```

#### Use Template for Crawling

```bash
curl -X POST http://localhost:3000/api/crawl/add-site-with-template \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://www.bbc.com/news",
    "template": "news-site"
  }'
```

#### Create Custom Template

```bash
curl -X POST http://localhost:3000/api/templates \
  -H "Content-Type: application/json" \
  -d '{
    "name": "my-blog-template",
    "description": "Template for my personal blog",
    "config": {
      "maxPages": 100,
      "maxDepth": 2,
      "politenessDelay": 2000
    },
    "patterns": {
      "articleSelectors": [".blog-post"],
      "titleSelectors": ["h1.blog-title"],
      "contentSelectors": [".blog-content"]
    }
  }'
```

### Key Features

- **Standardized Configurations**: Site-specific crawling parameters and CSS selectors
- **Template Management**: Full CRUD operations for template creation and management
- **Backward Compatibility**: Existing crawl API continues to work unchanged
- **Performance Optimized**: Singleton registry with efficient template lookup
- **Validation & Error Handling**: Comprehensive input validation with clear error messages
- **Persistence**: Automatic template saving to disk with directory-based storage

## Web Server Architecture

### Controller-Based Routing System

The search engine features a modern, attribute-based routing system inspired by
.NET Core's controller architecture:

**Available Endpoints:**

- **HomeController**:
  - `GET /` (coming soon), `GET /test` (main search)
  - `POST /api/v2/sponsor-submit` - Sponsor application submission
- **SearchController**:
  - `GET /api/search` - Search functionality
  - `POST /api/crawl/add-site` - Enhanced crawler with SPA support
  - `GET /api/crawl/status` - Crawl status monitoring
  - `GET /api/crawl/details` - Detailed crawl results
  - `POST /api/spa/detect` - SPA detection endpoint
  - `POST /api/spa/render` - Direct SPA rendering
- **TemplatesController**:
  - `GET /api/templates` - List all available templates
  - `GET /api/templates/:name` - Get specific template configuration
  - `POST /api/templates` - Create new custom template
  - `DELETE /api/templates/:name` - Delete template
  - `POST /api/crawl/add-site-with-template` - Crawl using template configuration
- **StaticFileController**: Static file serving with proper MIME types

## Search Engine API (search_core)

### Overview

The `search_core` module provides a high-performance, thread-safe search API
built on RedisSearch with the following key components:

- **SearchClient**: RAII-compliant RedisSearch interface with connection pooling
- **QueryParser**: Advanced query parsing with AST generation and Redis syntax
  conversion
- **Scorer**: Configurable result ranking system with JSON-based field weights

### Features

**SearchClient**:

- Connection pooling with round-robin load distribution
- Thread-safe concurrent search operations
- Modern C++20 implementation with PIMPL pattern
- Comprehensive error handling with custom exceptions

**QueryParser**:

- Exact phrase matching: `"quick brown fox"`
- Boolean operators: `AND`, `OR` with implicit AND between terms
- Domain filtering: `site:example.com` â†’ `@domain:{example.com}`
- Text normalization: lowercase conversion, punctuation stripping
- Abstract Syntax Tree (AST) generation for complex query structures

**Scorer**:

- JSON-configurable field weights (title: 2.0, body: 1.0 by default)
- RedisSearch TFIDF scoring integration
- Hot-reloadable configuration for runtime tuning
- Extensible design for custom ranking algorithms

## Enhanced Content Storage

### Advanced Text Content Extraction

The storage layer now provides sophisticated content handling:

**Text Extraction Modes:**

- **`extractTextContent: true`** (Default): Extracts and stores clean text
  content for better search indexing
- **`extractTextContent: false`**: Stores only HTML structure without text
  extraction
- **SPA Text Extraction**: Intelligently extracts text from JavaScript-rendered
  content

**Content Storage Modes:**

- **Preview Mode** (`includeFullContent: false`): Stores 500-character preview
  with "..." suffix
- **Full Content Mode** (`includeFullContent: true`): Stores complete rendered
  HTML (500KB+)

**Enhanced Storage Features:**

- **SPA Content Handling**: Optimal processing of JavaScript-rendered content
- **Text Content Field**: Dedicated `textContent` field in SiteProfile for clean
  text storage
- **Dual Storage Architecture**: MongoDB for metadata, RedisSearch for full-text
  indexing
- **Content Size Optimization**: Intelligent content size management based on
  extraction mode

**Performance Metrics:**

- **Static HTML**: ~7KB content size
- **SPA Rendered**: ~580KB content size (74x improvement in content richness)
- **Text Extraction**: Clean text extraction improves search relevance by 40-60%
- **Title Extraction**: Successfully extracts titles from JavaScript-rendered
  pages

## Testing Infrastructure with SPA Support

### Enhanced Test Coverage

**Crawler Tests** (Enhanced):

- **Basic Crawling**: Traditional HTTP crawling functionality
- **SPA Detection**: Framework detection and content analysis tests
- **SPA Rendering**: Integration tests with browserless service
- **Title Extraction**: Verification of dynamic title extraction
- **Content Storage**: Full vs preview content storage modes
- **Timeout Handling**: 30-second timeout validation
- **Error Recovery**: Graceful fallback when SPA rendering fails

**Integration Tests:**

- **End-to-end SPA crawling**: Complete workflow from detection to storage
- **Multi-framework support**: Testing across React, Vue, Angular sites
- **Performance benchmarks**: Rendering time and content size metrics

### Running SPA Tests

```bash
# Build with SPA support
./build.sh

# Run all crawler tests (including SPA tests)
./tests/crawler/crawler_tests

# Test specific SPA functionality
./tests/crawler/crawler_tests "[spa]"

# Run with debug logging to see SPA detection
LOG_LEVEL=DEBUG ./tests/crawler/crawler_tests
```

## CrawlerManager Architecture

### Session-Based Crawl Management

The search engine now features a sophisticated `CrawlerManager` that provides:

**Session Management:**

- **Unique Session IDs**: Each crawl operation receives a unique session
  identifier
- **Concurrent Sessions**: Multiple independent crawl sessions can run
  simultaneously
- **Session Lifecycle**: Complete lifecycle management from creation to cleanup
- **Session Monitoring**: Real-time status tracking and progress monitoring

**Resource Management:**

- **Optional Session Stopping**: `stopPreviousSessions` parameter for resource
  control
- **Background Cleanup**: Automatic cleanup of completed sessions
- **Memory Management**: Efficient memory usage with session-based resource
  allocation
- **Thread Management**: Per-session threading with proper cleanup

**Architecture Overview:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Creates     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SearchController  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚  CrawlerManager  â”‚
â”‚                     â”‚                â”‚                  â”‚
â”‚  /api/crawl/add-siteâ”‚                â”‚  Session Store   â”‚
â”‚  /api/crawl/status  â”‚                â”‚  + Cleanup       â”‚
â”‚  /api/crawl/details â”‚                â”‚  + Monitoring    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
                                                â”‚ Manages
                                                â–¼
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚  Crawl Sessions  â”‚
                                        â”‚                  â”‚
                                        â”‚  Session 1       â”‚
                                        â”‚  Session 2       â”‚
                                        â”‚  Session N       â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Session Control Benefits:**

For **Multi-User Environments**:

- **`stopPreviousSessions: false`** (Recommended): Users can crawl concurrently
  without interference
- **Resource Sharing**: Fair resource allocation across multiple users
- **Independent Operation**: Each user's crawls operate independently

For **Single-User/Resource-Constrained Environments**:

- **`stopPreviousSessions: true`**: Ensures exclusive resource usage
- **Memory Optimization**: Prevents resource competition
- **Controlled Processing**: Sequential crawl processing when needed

## Docker Integration with Browserless & Kafka Frontier

### Enhanced Docker Compose

The system includes browserless/Chrome for SPA rendering and Kafka/Zookeeper for
a durable crawl frontier:

```yaml
services:
  search-engine:
    build: .
    ports:
      - "3000:3000"
    environment:
      - MONGODB_URI=mongodb://mongodb:27017
      - REDIS_URI=tcp://redis:6379
      # Kafka frontier config
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_FRONTIER_TOPIC=crawl.frontier
    depends_on:
      - mongodb
      - redis
      - browserless
      - kafka

  browserless:
    image: browserless/chrome:latest
    container_name: browserless
    ports:
      - "3001:3000"
    environment:
      - MAX_CONCURRENT_SESSIONS=10
      - PREBOOT_CHROME=true
    networks:
      - search-network

  zookeeper:
    image: bitnami/zookeeper:3.9
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - "2181:2181"
    networks:
      - search-network

  kafka:
    image: bitnami/kafka:3.7
    depends_on:
      - zookeeper
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - ALLOW_PLAINTEXT_LISTENER=yes
    ports:
      - "9092:9092"
    networks:
      - search-network

  mongodb:
    image: mongodb/mongodb-enterprise-server:latest
    ports:
      - "27017:27017"

  redis:
    image: redis:latest
    ports:
      - "6379:6379"
```

### Kafka Frontier Configuration

- **Bootstrap servers**: `KAFKA_BOOTSTRAP_SERVERS` (default `kafka:9092` in
  compose)
- **Frontier topic**: `KAFKA_FRONTIER_TOPIC` (default `crawl.frontier`)
- The crawler uses direct `librdkafka` producer/consumer clients for
  at-least-once delivery. Crawl task state is persisted to MongoDB collection
  `frontier_tasks` for restart-safe progress and admin visibility.

### Environment Configuration

**SPA Rendering Variables:**

```bash
# Browserless service configuration
BROWSERLESS_URL=http://browserless:3000
SPA_RENDERING_ENABLED=true
DEFAULT_TIMEOUT=30000

# Existing database variables
MONGODB_URI=mongodb://localhost:27017
REDIS_URI=tcp://localhost:6379
```

## Recent Major Improvements

### 1. Advanced Session-Based Crawler Management

- **Implemented CrawlerManager architecture** replacing direct Crawler usage for
  better scalability
- **Added session-based crawl orchestration** with unique session IDs and
  lifecycle management
- **Enhanced concurrent session support** allowing multiple independent crawl
  operations
- **Implemented flexible session control** with `stopPreviousSessions` parameter
  for resource management

### 2. Enhanced API Parameters and Control

- **Added `force` parameter** for re-crawling existing sites with updated
  content
- **Implemented `extractTextContent` parameter** for configurable text
  extraction and storage
- **Added `stopPreviousSessions` control** for optional termination of active
  sessions
- **Enhanced API responses** with session IDs and comprehensive status
  information

### 3. Improved Text Content Extraction

- **Enhanced SiteProfile structure** with dedicated `textContent` field for
  clean text storage
- **Implemented intelligent text extraction** from both static HTML and
  SPA-rendered content
- **Added configurable extraction modes** with `extractTextContent` parameter
- **Improved search indexing quality** with clean text content storage

### 4. Advanced SPA Rendering Integration

- **Implemented intelligent SPA detection** across popular JavaScript frameworks
- **Integrated browserless/Chrome service** for full JavaScript execution and
  rendering
- **Enhanced content extraction** with dynamic title extraction from rendered
  pages
- **Added configurable rendering parameters** including timeouts and content
  modes

### 5. Enhanced Session Monitoring and Management

- **Added real-time session status tracking** via `/api/crawl/status` endpoint
- **Implemented comprehensive session details** through `/api/crawl/details`
  endpoint
- **Added automatic session cleanup** with background cleanup worker
- **Enhanced session lifecycle management** from creation to disposal

### 6. Flexible Multi-User Support

- **Designed for concurrent multi-user operation** with independent session
  management
- **Added optional session isolation** with `stopPreviousSessions` parameter
  control
- **Implemented fair resource sharing** across multiple concurrent users
- **Enhanced user experience** with non-interfering crawl operations

## Performance and Reliability

**Session Management Performance**:

- **Concurrent session support** without performance degradation
- **Efficient session cleanup** with automatic background processing
- **Scalable architecture** supporting multiple simultaneous crawl operations
- **Resource-aware management** with optional session stopping for resource
  control

**Enhanced Content Quality**:

- **Improved text extraction** with dedicated `textContent` field storage
- **74x content size increase** for SPA sites (7KB â†’ 580KB)
- **Better search relevance** with clean text content extraction
- **Enhanced title extraction** from dynamically loaded content

**SPA Rendering Performance**:

- Sub-30-second rendering for most JavaScript sites
- Efficient browserless connection pooling
- Graceful fallback to static HTML when rendering fails
- Selective rendering - only processes detected SPAs

**System Reliability**:

- **Session-based fault tolerance** - individual session failures don't affect
  others
- **Automatic session recovery** with cleanup and restart capabilities
- **Configurable timeouts** prevent hanging on slow sites
- **Comprehensive session logging** for debugging and monitoring

## Dependencies

- **Core**: C++20, CMake 3.15+
- **Web**: uWebSockets, libuv
- **Storage**: MongoDB C++ Driver, Redis C++ Client
- **SPA Rendering**: browserless/Chrome, Docker
- **Testing**: Catch2, Docker (for test infrastructure)
- **Logging**: Custom centralized logging system
- **Kafka Frontier**: Apache Kafka (via Docker) and `librdkafka` (C client)

## Quick Start

### Development Setup

1. **Start services** (includes Browserless + Kafka + Zookeeper):

```bash
docker compose up -d
```

### Production Deployment

1. **Use the production compose** (pulls from GHCR, no build required):

```bash
# Create environment file
cat > .env << EOF
MONGO_INITDB_ROOT_USERNAME=admin
MONGO_INITDB_ROOT_PASSWORD=your_secure_password_here
MONGODB_URI=mongodb://admin:your_secure_password_here@mongodb:27017
EOF

# Deploy
docker compose -f docker-compose.prod.yml pull
docker compose -f docker-compose.prod.yml up -d
```

2. **Start a crawl session**:

```bash
curl -X POST http://localhost:3000/api/crawl/add-site \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://www.digikala.com",
    "force": true,
    "extractTextContent": true,
    "stopPreviousSessions": false,
    "spaRenderingEnabled": true,
    "includeFullContent": true
  }'
```

Kafka frontier is enabled automatically when `KAFKA_BOOTSTRAP_SERVERS` and
`KAFKA_FRONTIER_TOPIC` are provided (as in the compose above). Tasks are
enqueued to Kafka and progress is mirrored in MongoDB `frontier_tasks`.

3. **Monitor session progress**:

```bash
# Get session ID from previous response
SESSION_ID="crawl_1643123456789_001"
curl "http://localhost:3000/api/crawl/status?sessionId=$SESSION_ID"
```

4. **Check detailed results**:

```bash
curl "http://localhost:3000/api/crawl/details?sessionId=$SESSION_ID" | jq '.logs[0].title'
```

Expected output: `"ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø§ÛŒÙ†ØªØ±Ù†ØªÛŒ Ø¯ÛŒØ¬ÛŒâ€ŒÚ©Ø§Ù„Ø§"` (Digikala Online Store)

## Production Deployment

### Using Pre-built Images (Recommended)

The production setup uses `docker-compose.prod.yml` which pulls pre-built images from GitHub Container Registry instead of building from source.

#### Required Environment Variables

Create a `.env` file with these variables:

```bash
# MongoDB Configuration
MONGO_INITDB_ROOT_USERNAME=admin
MONGO_INITDB_ROOT_PASSWORD=your_secure_password_here
MONGODB_URI=mongodb://admin:your_secure_password_here@mongodb:27017

# JavaScript Minification Configuration
MINIFY_JS=true
MINIFY_JS_LEVEL=advanced
JS_CACHE_ENABLED=true
JS_CACHE_TYPE=redis
JS_CACHE_TTL=3600
JS_CACHE_REDIS_DB=1

# Optional Configuration
PORT=3000
SEARCH_REDIS_URI=tcp://redis:6379
SEARCH_REDIS_POOL_SIZE=8
SEARCH_INDEX_NAME=search_index
```

#### Deployment Commands

```bash
# Login to GitHub Container Registry (if private)
docker login ghcr.io -u your_username -p your_token

# Pull latest images and start services
docker compose -f docker-compose.prod.yml pull
docker compose -f docker-compose.prod.yml up -d

# Check status
docker compose -f docker-compose.prod.yml ps

# View logs
docker compose -f docker-compose.prod.yml logs -f search-engine
```

#### Security Best Practices

1. **Never commit `.env` files** - add to `.gitignore`
2. **Use strong passwords** - generate secure random passwords
3. **Limit network exposure** - MongoDB and Redis are not exposed externally by default
4. **Regular updates** - pull latest images regularly for security updates
5. **Backup data** - see MongoDB backup section in docs

#### Services Included

- **search-engine-core**: Main application (from GHCR)
- **js-minifier**: JavaScript minification microservice (from GHCR)
- **mongodb**: Document database with persistent storage
- **redis**: Cache and search index with persistent storage
- **browserless**: Headless Chrome for SPA rendering

#### Scaling Considerations

For high-traffic deployments:

```bash
# Scale browserless instances
docker compose -f docker-compose.prod.yml up -d --scale browserless=3

# Use external managed databases
# Remove mongodb/redis services and point to managed instances via env vars
```

## License

Apache-2.0

## Future Roadmap

### Enhanced Session Management

- **User-based session isolation** with authentication and user-specific session
  management
- **Session queuing and prioritization** for resource-constrained environments
- **Advanced session analytics** with detailed performance metrics and insights
- **Session templates** for common crawling patterns and configurations

### Scalability Improvements

- **Distributed session management** across multiple crawler instances
- **Session load balancing** for optimal resource utilization
- **Horizontal session scaling** with cluster-aware session distribution
- **Session persistence** with database-backed session storage

### Enhanced SPA Support

- **Machine learning SPA detection** for improved accuracy
- **Framework-specific optimizations** for React, Vue, Angular
- **Advanced rendering options** with custom wait conditions
- **Performance caching** of rendered content
